{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f948d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/transformers-2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "#Set to avoid warning messages.\n",
    "transformers.logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eca074",
   "metadata": {},
   "source": [
    "## 04.02. Loading a Hugging Face Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a6b809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset poem_sentiment (/Users/linkedin/.cache/huggingface/datasets/poem_sentiment/default/1.0.0/4e44428256d42cdde0be6b3db1baa587195e91847adabf976e4f9454f6a82099)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 438.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 892\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 105\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 104\n",
      "    })\n",
      "})\n",
      "{'id': [20, 21, 22, 23, 24], 'verse_text': [\"as o'er the earth it wanders wide,\", 'how hearts were answering to his own,', 'glad on its stone-built hearth; and thorough the wide-mouthed smoke-flue', 'sees the clouds reel and roll above our head,', '’tis to behold his vengeance for my son.'], 'label': [2, 1, 2, 2, 0]}\n",
      "\n",
      "Sentiment Labels used ['negative', 'positive', 'no_impact', 'mixed']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#Use pretrained model checkpoint from Huggingface\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "#Use pre-labeled dataset from huggingface\n",
    "dataset_name= \"poem_sentiment\"\n",
    "\n",
    "poem_sentiments = load_dataset(dataset_name)\n",
    "\n",
    "#Apache Arrow format\n",
    "print(poem_sentiments)\n",
    "print(poem_sentiments[\"test\"][20:25])\n",
    "\n",
    "print(\"\\nSentiment Labels used\", \n",
    "      poem_sentiments[\"train\"].features.get(\"label\").names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54018a23",
   "metadata": {},
   "source": [
    "## 04.03. Encoding and pre-processing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb057a9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/linkedin/.cache/huggingface/datasets/poem_sentiment/default/1.0.0/4e44428256d42cdde0be6b3db1baa587195e91847adabf976e4f9454f6a82099/cache-e80e4ec2cbfe3b03.arrow\n",
      "Loading cached processed dataset at /Users/linkedin/.cache/huggingface/datasets/poem_sentiment/default/1.0.0/4e44428256d42cdde0be6b3db1baa587195e91847adabf976e4f9454f6a82099/cache-781d7903f5c80b58.arrow\n",
      "Loading cached processed dataset at /Users/linkedin/.cache/huggingface/datasets/poem_sentiment/default/1.0.0/4e44428256d42cdde0be6b3db1baa587195e91847adabf976e4f9454f6a82099/cache-826927031abc85e4.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': [0, 1, 2, 3, 4], 'verse_text': ['with pale blue berries. in these peaceful shades--', 'it flows so long as falls the rain,', 'and that is why, the lonesome day,', 'when i peruse the conquered fame of heroes, and the victories of mighty generals, i do not envy the generals,', 'of inward strife for truth and liberty.'], 'label': [1, 2, 0, 3, 3], 'input_ids': [[101, 2007, 5122, 2630, 22681, 1012, 1999, 2122, 9379, 13178, 1011, 1011, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2009, 6223, 2061, 2146, 2004, 4212, 1996, 4542, 1010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1998, 2008, 2003, 2339, 1010, 1996, 10459, 14045, 2154, 1010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2043, 1045, 7304, 3366, 1996, 11438, 4476, 1997, 7348, 1010, 1998, 1996, 9248, 1997, 10478, 11593, 1010, 1045, 2079, 2025, 21103, 1996, 11593, 1010, 102, 0, 0], [101, 1997, 20546, 27865, 2005, 3606, 1998, 7044, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "#Encoding text\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "db_tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return db_tokenizer(batch[\"verse_text\"], \n",
    "                        padding=True, \n",
    "                        truncation=True)\n",
    "\n",
    "enc_poem_sentiment = poem_sentiments.map(\n",
    "                        tokenize, \n",
    "                        batched=True, \n",
    "                        batch_size=None)\n",
    "\n",
    "print(enc_poem_sentiment[\"train\"][0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5775b7cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text : it flows so long as falls the rain,\n",
      "\n",
      "Input Map : [101, 2009, 6223, 2061, 2146, 2004, 4212, 1996, 4542, 1010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Attention Mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Total tokens:  28\n",
      "Non Zero tokens:  11\n",
      "Attention = 1:  11\n"
     ]
    }
   ],
   "source": [
    "#Explore input IDs and Attention Mask\n",
    "\n",
    "print(\"Text :\", \n",
    "      enc_poem_sentiment[\"train\"][1].get(\"verse_text\"))\n",
    "print(\"\\nInput Map :\", \n",
    "      enc_poem_sentiment[\"train\"][1].get(\"input_ids\"))\n",
    "print(\"\\nAttention Mask :\", \n",
    "      enc_poem_sentiment[\"train\"][1].get(\"attention_mask\"))\n",
    "\n",
    "print(\"\\nTotal tokens: \", \n",
    "      len(enc_poem_sentiment[\"train\"][1].get(\"input_ids\")))\n",
    "print(\"Non Zero tokens: \", \n",
    "      len(list(filter( \n",
    "        lambda x :x > 0, \n",
    "          enc_poem_sentiment[\"train\"][1].get(\"input_ids\")))))\n",
    "print(\"Attention = 1: \", \n",
    "      len(list(filter( \n",
    "        lambda x :x > 0, \n",
    "          enc_poem_sentiment[\"train\"][1].get(\"attention_mask\")))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4a924c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column Names :  ['id', 'verse_text', 'label', 'input_ids', 'attention_mask']\n",
      "\n",
      "Features :  {'id': Value(dtype='int32', id=None), 'verse_text': Value(dtype='string', id=None), 'label': ClassLabel(names=['negative', 'positive', 'no_impact', 'mixed'], id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "#Separate training and validation sets\n",
    "training_dataset = enc_poem_sentiment[\"train\"]\n",
    "validation_dataset=enc_poem_sentiment[\"validation\"]\n",
    "\n",
    "print(\"\\nColumn Names : \",training_dataset.column_names)\n",
    "print(\"\\nFeatures : \",training_dataset.features)\n",
    "\n",
    "labels = training_dataset.features.get(\"label\")\n",
    "num_labels=len(labels.names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3faefe9",
   "metadata": {},
   "source": [
    "## 04.04. Creating the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2094ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 30522,\n",
       " 'max_position_embeddings': 512,\n",
       " 'sinusoidal_pos_embds': False,\n",
       " 'n_layers': 6,\n",
       " 'n_heads': 12,\n",
       " 'dim': 768,\n",
       " 'hidden_dim': 3072,\n",
       " 'dropout': 0.1,\n",
       " 'attention_dropout': 0.1,\n",
       " 'activation': 'gelu',\n",
       " 'initializer_range': 0.02,\n",
       " 'qa_dropout': 0.1,\n",
       " 'seq_classif_dropout': 0.2,\n",
       " 'return_dict': True,\n",
       " 'output_hidden_states': False,\n",
       " 'output_attentions': False,\n",
       " 'torchscript': False,\n",
       " 'torch_dtype': None,\n",
       " 'use_bfloat16': False,\n",
       " 'tf_legacy_loss': False,\n",
       " 'pruned_heads': {},\n",
       " 'tie_word_embeddings': True,\n",
       " 'is_encoder_decoder': False,\n",
       " 'is_decoder': False,\n",
       " 'cross_attention_hidden_size': None,\n",
       " 'add_cross_attention': False,\n",
       " 'tie_encoder_decoder': False,\n",
       " 'max_length': 20,\n",
       " 'min_length': 0,\n",
       " 'do_sample': False,\n",
       " 'early_stopping': False,\n",
       " 'num_beams': 1,\n",
       " 'num_beam_groups': 1,\n",
       " 'diversity_penalty': 0.0,\n",
       " 'temperature': 1.0,\n",
       " 'top_k': 50,\n",
       " 'top_p': 1.0,\n",
       " 'typical_p': 1.0,\n",
       " 'repetition_penalty': 1.0,\n",
       " 'length_penalty': 1.0,\n",
       " 'no_repeat_ngram_size': 0,\n",
       " 'encoder_no_repeat_ngram_size': 0,\n",
       " 'bad_words_ids': None,\n",
       " 'num_return_sequences': 1,\n",
       " 'chunk_size_feed_forward': 0,\n",
       " 'output_scores': False,\n",
       " 'return_dict_in_generate': False,\n",
       " 'forced_bos_token_id': None,\n",
       " 'forced_eos_token_id': None,\n",
       " 'remove_invalid_values': False,\n",
       " 'exponential_decay_length_penalty': None,\n",
       " 'suppress_tokens': None,\n",
       " 'begin_suppress_tokens': None,\n",
       " 'architectures': ['DistilBertForMaskedLM'],\n",
       " 'finetuning_task': None,\n",
       " 'id2label': {0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2', 3: 'LABEL_3'},\n",
       " 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3},\n",
       " 'tokenizer_class': None,\n",
       " 'prefix': None,\n",
       " 'bos_token_id': None,\n",
       " 'pad_token_id': 0,\n",
       " 'eos_token_id': None,\n",
       " 'sep_token_id': None,\n",
       " 'decoder_start_token_id': None,\n",
       " 'task_specific_params': None,\n",
       " 'problem_type': None,\n",
       " '_name_or_path': 'distilbert-base-uncased',\n",
       " 'transformers_version': '4.25.1',\n",
       " 'model_type': 'distilbert',\n",
       " 'tie_weights_': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "#Load transformer checkpoint from huggingface\n",
    "sentiment_model = (TFAutoModelForSequenceClassification\n",
    "            .from_pretrained(model_name, num_labels=num_labels))\n",
    "\n",
    "sentiment_model.get_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f07af1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3076      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,956,548\n",
      "Trainable params: 66,956,548\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Freeze the first layer if needed\n",
    "sentiment_model.layers[0].trainable = True\n",
    "\n",
    "#Add/remove layers if needed.\n",
    "#sentiment_model.layers [append()/insert()/remove()]\n",
    "\n",
    "print(sentiment_model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575dca13",
   "metadata": {},
   "source": [
    "# 04.05. Training the Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72bdd464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "14/14 [==============================] - 72s 4s/step - loss: 0.2459 - sparse_categorical_accuracy: 0.9204 - val_loss: 0.5771 - val_sparse_categorical_accuracy: 0.8571\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 59s 4s/step - loss: 0.1215 - sparse_categorical_accuracy: 0.9675 - val_loss: 0.4355 - val_sparse_categorical_accuracy: 0.8762\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 59s 4s/step - loss: 0.0808 - sparse_categorical_accuracy: 0.9753 - val_loss: 0.4960 - val_sparse_categorical_accuracy: 0.8857\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 59s 4s/step - loss: 0.0474 - sparse_categorical_accuracy: 0.9899 - val_loss: 0.5431 - val_sparse_categorical_accuracy: 0.8762\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 60s 4s/step - loss: 0.0249 - sparse_categorical_accuracy: 0.9955 - val_loss: 0.5467 - val_sparse_categorical_accuracy: 0.8857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8119af8160>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using features from a pretrained model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size=64\n",
    "tokenizer_columns = db_tokenizer.model_input_names\n",
    "\n",
    "# Convert to TF tensors\n",
    "train_dataset = training_dataset.to_tf_dataset(\n",
    "    columns=tokenizer_columns, label_cols=[\"label\"], shuffle=True,\n",
    "    batch_size=batch_size)\n",
    "val_dataset = validation_dataset.to_tf_dataset(\n",
    "    columns=tokenizer_columns, label_cols=[\"label\"], shuffle=False,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "sentiment_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy())\n",
    "\n",
    "sentiment_model.fit(train_dataset, \n",
    "                    validation_data=val_dataset, \n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb86c123",
   "metadata": {},
   "source": [
    "## 04.06. Predicting Sentiment with the Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de686bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    infer: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 189.46ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec={'input_ids': TensorSpec(shape=(None, 17), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, 17), dtype=tf.int64, name=None)}>\n",
      "1/1 [==============================] - 0s 138ms/step\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset,DatasetDict\n",
    "\n",
    "#Input data for interference to predict sentiment\n",
    "# the \"label\" array is not needed for inference, but added to provide true labels for comparison\n",
    "infer_data = {'id':[0,1], \n",
    "             'verse_text':['and be glad in the summer morning when the kindred ride on their way', \n",
    "                           'how hearts were answering to his own'],\n",
    "             'label':[1,0]}\n",
    "\n",
    "infer_dataset = Dataset.from_dict(infer_data)\n",
    "\n",
    "ds_dict=DatasetDict()\n",
    "ds_dict[\"infer\"] = infer_dataset\n",
    "\n",
    "print(ds_dict)\n",
    "\n",
    "#Encode the dataset, similar to training\n",
    "enc_dataset=ds_dict.map(tokenize, batched=True, batch_size=None)\n",
    "\n",
    "#Convert to Tensors\n",
    "infer_final_dataset = enc_dataset[\"infer\"].to_tf_dataset(\n",
    "    columns=tokenizer_columns,  shuffle=True,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "print(infer_final_dataset)\n",
    "\n",
    "#Predict with the model\n",
    "predictions=sentiment_model.predict(infer_final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2531d52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.3346035,  4.137719 , -1.7129751, -1.0897871],\n",
       "       [-2.544504 ,  3.4136968, -0.5179995, -1.3775116]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67aef028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem = and be glad in the summer morning when the kindred ride on their way  Predicted= positive  True-Label= positive\n",
      "Poem = how hearts were answering to his own  Predicted= positive  True-Label= negative\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred_label_ids=np.argmax(predictions.logits, axis=1)\n",
    "\n",
    "for i in range(len(pred_label_ids)):\n",
    "    print(\"Poem =\", infer_data[\"verse_text\"][i], \n",
    "          \" Predicted=\",labels.names[pred_label_ids[i]], \n",
    "          \" True-Label=\",labels.names[infer_data[\"label\"][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3849e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
