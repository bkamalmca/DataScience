{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import boto3, re, sys, math, json, os, sagemaker, urllib.request\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "bucket_name = 'dgx-ds-use1-dev-landing-s3'\n",
    "prefix = 'kamal/OrderTAT-tf'\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key = 'kamal/input/order_data_prep_job1.csv' \n",
    "data_location = 's3://{}/{}'.format(bucket_name, data_key) \n",
    "\n",
    "try:\n",
    "  df = pd.read_csv(data_location, index_col=0)\n",
    "  print('Success: Data loaded into dataframe.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[((df['TAT_HOUR']>2000) & (df['TAT_HOUR']<3000))]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target encoding Order code\n",
    "#df['TAT_HOUR'].mean()\n",
    "df['ORDER_ENC'] = df.groupby('ORDER_CODE_N')['TAT_HOUR'].transform('mean')\n",
    "df['ORDER_ENC'] = round((0.6 * df['ORDER_ENC']) + (0.4 * df['TAT_HOUR'].mean()),2)\n",
    "#df['Distance'] = df['Distance']/1000\n",
    "df['Distance'] = zscore(df['Distance'])\n",
    "df['ORDER_ENC'] = zscore(df['ORDER_ENC'])\n",
    "df.drop('ORDER_CODE_N', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try removing outliers\n",
    "#df=df[~((df['TAT_HOUR']>df['TAT_HOUR'].quantile(.998)) | (df['TAT_HOUR']<0))] #loss reduced from 7000 to 900\n",
    "df=df[~((df['TAT_HOUR']>df['TAT_HOUR'].quantile(.99)) | (df['TAT_HOUR']<0))] #loss reduced from 7000 to 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding order code and perf lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "model_data = pd.get_dummies(df, drop_first = True)\n",
    "model_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key = 'kamal/input/order_data_tf.csv' \n",
    "data_location = 's3://{}/{}'.format(bucket_name, data_key) \n",
    "print(data_location)\n",
    "\n",
    "model_data.to_csv(data_location, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv(data_location, nrows=10) #, index_col=0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.20.0\", role=role, instance_type=\"ml.m5.4xlarge\", instance_count=1\n",
    ")\n",
    "\n",
    "input_data=data_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from datetime import datetime\n",
    "\n",
    "dt_string = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "sklearn_processor.run(\n",
    "    job_name=\"kamal-tf-fileprep-\" + dt_string,\n",
    "    code=\"OrderTAT-tf-fileprep.py\",\n",
    "    inputs=[ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", source=\"/opt/ml/processing/train\", destination=\"s3://dgx-ds-use1-dev-landing-s3/kamal/tf-keras-orderTAT/data/train\"),\n",
    "        ProcessingOutput(output_name=\"test_data\", source=\"/opt/ml/processing/test\", destination=\"s3://dgx-ds-use1-dev-landing-s3/kamal/tf-keras-orderTAT/data/test\"),\n",
    "    ],\n",
    "    arguments=[\"--train-test-split-ratio\", \"0.2\"],\n",
    ")\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description[\"ProcessingOutputConfig\"]\n",
    "for output in output_config[\"Outputs\"]:\n",
    "    if output[\"OutputName\"] == \"train_data\":\n",
    "        preprocessed_training_data = output[\"S3Output\"][\"S3Uri\"]\n",
    "    if output[\"OutputName\"] == \"test_data\":\n",
    "        preprocessed_training_data = output[\"S3Output\"][\"S3Uri\"]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#large not working for 2.4M\n",
    "model_data = model_data.sample(n=1000000, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_columns = model_data.columns\n",
    "print(x_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy - Classification\n",
    "x = model_data[x_columns[1:]].values\n",
    "y = model_data[x_columns[:1]].values\n",
    "print(\"x,y shape\",x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT run for fileprep processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT run for fileprep processing\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(), 'data/train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(os.getcwd(), 'data/test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "csv_test_dir = os.path.join(os.getcwd(), 'data/csv-test')\n",
    "os.makedirs(csv_test_dir, exist_ok=True)\n",
    "\n",
    "np.save(os.path.join(train_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(train_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(test_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(test_dir, 'y_test.npy'), y_test)\n",
    "np.savetxt(os.path.join(csv_test_dir, 'csv-test.csv'), np.array(x_test[:1], dtype=np.int32), fmt='%d', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "s3_prefix = 'kamal/tf-keras-orderTAT'\n",
    "bucket = 'dgx-ds-use1-dev-landing-s3'\n",
    "\n",
    "traindata_s3_prefix = '/{}/data/train'.format(s3_prefix)\n",
    "testdata_s3_prefix = '/{}/data/test'.format(s3_prefix)\n",
    "\n",
    "train_s3 = 's3://' + bucket + traindata_s3_prefix\n",
    "test_s3 = 's3://' + bucket + testdata_s3_prefix\n",
    "\n",
    "inputs = {'train': train_s3, 'test': test_s3}\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT run\n",
    "train_s3 = sagemaker.Session().upload_data(path='./data/train/', bucket=bucket, key_prefix=traindata_s3_prefix)\n",
    "test_s3 = sagemaker.Session().upload_data(path='./data/test/', bucket=bucket, key_prefix=testdata_s3_prefix)\n",
    "\n",
    "inputs = {'train':train_s3, 'test': test_s3}\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "model_dir = '/opt/ml/model'\n",
    "train_instance_type = 'ml.m5.4xlarge'\n",
    "hyperparameters = {'epochs': 10, 'batch_size': 16, 'learning_rate': 0.1}\n",
    "\n",
    "estimator = TensorFlow(\n",
    "                       #git_config=git_config,\n",
    "                       #source_dir='tf-sentiment-script-mode',\n",
    "                       entry_point='OrderTAT-tf-model1.py',\n",
    "                       model_dir=model_dir,\n",
    "                       instance_type=train_instance_type,\n",
    "                       instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       base_job_name='kamal-tf-orderTAT',\n",
    "                       framework_version='2.1',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "!aws s3 cp {estimator.model_data} ./model/model.tar.gz\n",
    "!tar -xzf ./model/model.tar.gz -C ./model\n",
    "\n",
    "with open('./model/history.p', \"r\") as f:\n",
    "    history = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_curves(history): \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
    "    ax = axes[0]\n",
    "    ax.plot(history['accuracy'], label='train')\n",
    "    ax.set(title='model accuracy', ylabel='accuracy', xlabel='epoch')\n",
    "    ax.legend()\n",
    "    ax = axes[1]\n",
    "    ax.plot(history['loss'], label='train')\n",
    "    ax.set(title='model loss', ylabel='loss', xlabel='epoch')\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    \n",
    "plot_training_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch prediction\n",
    "csvtestdata_s3_prefix = '{}/data/csv-test'.format(s3_prefix)\n",
    "csvtest_s3 = sagemaker.Session().upload_data(path='./data/csv-test/', bucket=bucket, key_prefix=csvtestdata_s3_prefix)\n",
    "print(csvtest_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = estimator.transformer(instance_count=1,                                    \n",
    "                                    #base_job_name='kamal-tf-sentiment-tfm',                                    \n",
    "                                    instance_type='ml.m5.4xlarge')\n",
    "\n",
    "transformer.transform(csvtest_s3, content_type='text/csv')\n",
    "print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch transform\n",
    "data_key = prefix + '/test' \n",
    "batch_input = 's3://{}/{}'.format(bucket_name, data_key) \n",
    "\n",
    "data_key2 = prefix + '/batch-predict' \n",
    "batch_output = 's3://{}/{}'.format(bucket_name, data_key2) \n",
    "\n",
    "print(batch_input)\n",
    "print(batch_output)\n",
    "\n",
    "print(x_train.shape, x_test.shape)\n",
    "\n",
    "xinp=pd.DataFrame(x_test)\n",
    "xinp.to_csv(batch_input + '/test.csv',index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "transformer = estimator.transformer(\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.2xlarge', \n",
    "    output_path=batch_output\n",
    ")\n",
    "\n",
    "transformer.transform(\n",
    "    data=batch_input, \n",
    "    data_type='S3Prefix',\n",
    "    content_type='text/csv', \n",
    "    split_type='Line'\n",
    ")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key2 = prefix + '/batch-predict' \n",
    "batch_output = 's3://{}/{}'.format(bucket_name, data_key2) \n",
    "print(batch_output)\n",
    "data_key2 = prefix + '/batch-predict/test.csv.out' \n",
    "batch_output = 's3://{}/{}'.format(bucket, data_key2) \n",
    "print(batch_output)\n",
    "pred_y = pd.read_csv(batch_output,nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp 's3://dgx-ds-use1-dev-landing-s3/kamal/OrderTAT-tf/batch-predict/test.csv.out' ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = 's3://{}'.format(bucket_name)\n",
    "print(batch_output)\n",
    "s3_client = boto3.client('s3')\n",
    "s3 = boto3.resource('s3')\n",
    "locations=['kamal/OrderTAT-tf/batch-predict/test.csv.out']\n",
    "for file in locations:\n",
    "    s3_client.download_file(batch_output, '/kamal/batch-predict/test.csv.out' , 'output')\n",
    "    #s3.Bucket(batch_output).download_file(file, os.path.basename(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score, r2_score, mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "model_score = explained_variance_score(y_train[:10], float(pred_y))\n",
    "print(\"Score = \", model_score)\n",
    "\n",
    "k = y_train.shape[1]\n",
    "n = len(test_data)\n",
    "MSE = mean_squared_error(test_data['TAT_HOUR'][1:], pred_y)\n",
    "MAE = mean_absolute_error(test_data['TAT_HOUR'][1:], pred_y)\n",
    "r2 = r2_score(test_data['TAT_HOUR'][1:], pred_y)\n",
    "adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)\n",
    "#print(\"MSE: %.2f\" % MSE)\n",
    "#print(\"RMSE: %.2f\" % (mse**(1/2.0)))\n",
    "print('RMSE = ',(MSE**(1/2.0)), '\\nMSE =',MSE, '\\nMAE =',MAE, '\\nR2 =', r2, '\\nAdjusted R2 =', adj_r2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs for small volume 100K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "# Build the neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "model.add(Dense(32, activation='relu')) # Hidden 2\n",
    "#model.add(Dense(10, activation='sigmoid')) # Hidden 2\n",
    "model.add(Dense(1)) # Output\n",
    "\n",
    "learning_rate=0.1\n",
    "optimizer = Adam(learning_rate)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "#model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
    "                        patience=5, verbose=1, mode='auto', \n",
    "                        restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train,validation_data=(X_test,y_test),\n",
    "          callbacks=[monitor],verbose=2,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m5.large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "test_data_p = test_data.sample(n=3000)\n",
    "test_data_array = test_data_p.drop(['TAT_HOUR'], axis=1).values #load the data into an array\n",
    "xgb_predictor.serializer = CSVSerializer() # set the serializer type\n",
    "predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array\n",
    "print(predictions_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score, r2_score, mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "model_score = explained_variance_score(test_data_p['TAT_HOUR'], predictions_array)\n",
    "print(\"Score = \",model_score)\n",
    "\n",
    "k = test_data_p.shape[1]\n",
    "n = len(test_data_p)\n",
    "MSE = mean_squared_error(test_data_p['TAT_HOUR'], predictions_array)\n",
    "MAE = mean_absolute_error(test_data_p['TAT_HOUR'], predictions_array)\n",
    "r2 = r2_score(test_data_p['TAT_HOUR'], predictions_array)\n",
    "adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)\n",
    "#print(\"MSE: %.2f\" % MSE)\n",
    "#print(\"RMSE: %.2f\" % (mse**(1/2.0)))\n",
    "print('RMSE =',(MSE**(1/2.0)), '\\nMSE =',MSE, '\\nMAE =',MAE, '\\nR2 =', r2, '\\nAdjusted R2 =', adj_r2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_ax = range(len(test_data_p['TAT_HOUR']))\n",
    "plt.scatter(test_data_p['TAT_HOUR'], predictions_array, s=5, color=\"blue\", label=\"original\")\n",
    "plt.plot(x_ax, predictions_array, lw=0.8, color=\"red\", label=\"predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete manually in dashboard\n",
    "#xgb_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bucket_to_delete = boto3.resource('s3').Bucket(bucket_name)\n",
    "#bucket_to_delete.objects.all().delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch transform\n",
    "data_key = prefix + '/test' \n",
    "batch_input = 's3://{}/{}'.format(bucket_name, data_key) \n",
    "\n",
    "data_key2 = prefix + '/batch-predict' \n",
    "batch_output = 's3://{}/{}'.format(bucket_name, data_key2) \n",
    "\n",
    "print(batch_input)\n",
    "print(batch_output)\n",
    "\n",
    "test_data.drop(['TAT_HOUR'], axis=1).to_csv(batch_input + '/test.csv',index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "transformer = xgb.transformer(\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.2xlarge', \n",
    "    output_path=batch_output\n",
    ")\n",
    "\n",
    "transformer.transform(\n",
    "    data=batch_input, \n",
    "    data_type='S3Prefix',\n",
    "    content_type='text/csv', \n",
    "    split_type='Line'\n",
    ")\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key2 = prefix + '/batch-predict/test.csv.out' \n",
    "batch_output = 's3://{}/{}'.format(bucket_name, data_key2) \n",
    "pred_y = pd.read_csv(batch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score, r2_score, mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "model_score = explained_variance_score(test_data['TAT_HOUR'][1:], pred_y)\n",
    "print(\"Score = \",model_score)\n",
    "\n",
    "k = test_data.shape[1]\n",
    "n = len(test_data)\n",
    "MSE = mean_squared_error(test_data['TAT_HOUR'][1:], pred_y)\n",
    "MAE = mean_absolute_error(test_data['TAT_HOUR'][1:], pred_y)\n",
    "r2 = r2_score(test_data['TAT_HOUR'][1:], pred_y)\n",
    "adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)\n",
    "#print(\"MSE: %.2f\" % MSE)\n",
    "#print(\"RMSE: %.2f\" % (mse**(1/2.0)))\n",
    "print('RMSE = ',(MSE**(1/2.0)), '\\nMSE =',MSE, '\\nMAE =',MAE, '\\nR2 =', r2, '\\nAdjusted R2 =', adj_r2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read model from s3\n",
    "modelx = sagemaker.model.Model(\n",
    "    image_uri=xgboost_container,\n",
    "    model_data ='s3://dgx-ds-use1-dev-landing-s3/kamal/OrderTAT-xgboost/output/sagemaker-xgboost-2021-10-26-03-08-13-506/output/model.tar.gz',\n",
    "    role=role)\n",
    "\n",
    "print(modelx)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
