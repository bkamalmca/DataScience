{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pandas\n",
    "!pip install geopy\n",
    "!pip install xgboost\n",
    "!pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date, datetime\n",
    "import random\n",
    "from geopy.distance import distance\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import holidays\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "outputs = []\n",
    "\n",
    "def clean_data(thisDf): \n",
    "    thisDf['ORDER_CODE'] = thisDf['ORDER_CODE'].astype(str)\n",
    "    # remove anything with a negative TAT\n",
    "#     thisDf = thisDf[(thisDf['TAT_HOUR']>=0) | (thisDf['TAT_HOUR'].isnull())]\n",
    "#     # remove anything that is over the 99th percentile\n",
    "#     thisDf = thisDf[(thisDf['TAT_HOUR']<=thisDf['TAT_HOUR'].quantile(.99)) | (thisDf['TAT_HOUR'].isnull())]\n",
    "    return thisDf\n",
    "    \n",
    "def initial_drop_columns(thisDf): \n",
    "    return thisDf.drop([\n",
    "        #'RECORD_ID',\n",
    "        'PERFORMING_LAB_NAME',\n",
    "        'BU_NAME',\n",
    "        'BU_LATITUDE',\n",
    "        'BU_LONGITUDE',\n",
    "        'ORDERING_LAB_CODE',\n",
    "        'ORDERING_LAB_NAME',     \n",
    "        'BILLING_LEGAL_ENTITY',\n",
    "        'ACCOUNT_NUMBER',\n",
    "        'ACCOUNT_NAME',\n",
    "        'ACCOUNT_STATE',\n",
    "        'ACCOUNT_ZIP_CODE',\n",
    "        'SPECIALTY_DESC',\n",
    "        'PHYSICIAN_NPI',\n",
    "        'PHYSICIAN_NAME',\n",
    "        'BILL_ONLY_INDICATOR',\n",
    "        'ORDER_UNIT_CODE',\n",
    "        'ORDER_NAME',\n",
    "        'ORDER_CODE_MNEMONIC',\n",
    "        'PUBLISHED_TAT',\n",
    "        'MAX_TAT',\n",
    "        'STAT_ROUTINE_INDICATOR'   \n",
    "    ], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def get_distances(thisDf):\n",
    "\n",
    "    def geo_distance(x):\n",
    "\n",
    "        try: \n",
    "            return round(distance( (x['PERFORMING_LAB_LATITUDE'], x['PERFORMING_LAB_LONGITUDE']),\n",
    "                               (x['ORDERING_LAB_LATITUDE'], x['ORDERING_LAB_LONGITUDE'])\n",
    "                           ).miles)\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    try:\n",
    "        thisDf['Distance'] = thisDf.progress_apply(geo_distance, axis=1)\n",
    "    except:\n",
    "        thisDf['Distance'] = thisDf.apply(geo_distance, axis=1)\n",
    "    return thisDf\n",
    "\n",
    "\n",
    "def clean_ordercode(thisDf):\n",
    "\n",
    "    def clean_code(x):\n",
    "        if (str(x['ORDER_CODE']).isnumeric()):\n",
    "            return str(x['ORDER_CODE'])\n",
    "        elif (str(x['ORDER_CODE'])[-3:] == 'NHD'):\n",
    "            return 'NHD' \n",
    "        elif (str(x['ORDER_CODE'])[-3:] == 'PTH'):\n",
    "            return 'PTH' \n",
    "        elif (str(x['ORDER_CODE'])[-4:] == 'CALC'):\n",
    "            return 'CALC' \n",
    "        elif (str(x['ORDER_CODE'])[-4:] == '6517'):\n",
    "            return 'ALBUM' \n",
    "        elif (str(x['ORDER_CODE'])[0:2] == 'AT'):\n",
    "            return 'TISSUE' \n",
    "        elif (str(x['ORDER_CODE'])[-3:] == 'CRL'):\n",
    "            return 'SBCRL' \n",
    "        elif (str(x['ORDER_CODE'])[0:8] == 'INTERPRE'):\n",
    "            return 'INTPR' \n",
    "        else:\n",
    "            return str(x['ORDER_CODE'])\n",
    "    \n",
    "    thisDf['ORDER_CODE_N'] = thisDf.apply(clean_code, axis=1)\n",
    "    return thisDf\n",
    "\n",
    "def do_concats(thisDf): \n",
    "    # df_t['Lab_Order'] = df_t['LAB_SYSTEM_ID'].astype(str) + df_t['ORDER_CODE'].astype(str)\n",
    "    # df_t['Performing_Lab'] = df_t['PERFORMING_LAB_SITE_TYPE'].astype(str) + df_t['PERFORMING_LAB_CODE'].astype(str)\n",
    "\n",
    "    return thisDf\n",
    "\n",
    "def update_add_on_exists(thisDf):\n",
    "    thisDf['Add_On_Exists'] =thisDf['ADD_ON_ORDER_DATE'].isnull()\n",
    "    thisDf['Add_On_Exists'] =thisDf['Add_On_Exists'].apply(lambda x: 0 if x is True else 1)\n",
    "    return thisDf\n",
    "\n",
    "def do_date_stuff(thisDf):\n",
    "#     def day_of_week(x):\n",
    "#         return x.day_name()\n",
    "\n",
    "    thisDf['COLLECTION_DATE'] = pd.to_datetime(thisDf['COLLECTION_DATE'])\n",
    "    thisDf['ACCESSION_DATE'] = pd.to_datetime(thisDf['ACCESSION_DATE'])\n",
    "    thisDf['Collection_DOW'] = thisDf['COLLECTION_DATE'].dt.day_name()\n",
    "    thisDf['Accession_DOW'] = thisDf['ACCESSION_DATE'].dt.day_name()\n",
    "    \n",
    "    #check to see if holiday\n",
    "    us_holidays = holidays.US()\n",
    "    \n",
    "    thisDf['Accession_is_Holiday'] = thisDf['ACCESSION_DATE'].apply(lambda x: x in us_holidays)\n",
    "    thisDf['Collection_is_Holiday'] = thisDf['COLLECTION_DATE'].apply(lambda x: x in us_holidays)\n",
    "    thisDf['Collection_is_Holiday'] = thisDf['Collection_is_Holiday'].apply(lambda x: 1 if x is True else 0)\n",
    "    thisDf['Accession_is_Holiday'] = thisDf['Accession_is_Holiday'].apply(lambda x: 1 if x is True else 0)\n",
    "    \n",
    "    \n",
    "    # get collection hour\n",
    "    thisDf['Collection_Hour'] = thisDf['COLLECTION_DATE'].dt.hour\n",
    "    \n",
    "    # do hours between collection/accession\n",
    "    thisDf['Hours_Collection_to_Accession'] = thisDf['ACCESSION_DATE'] - thisDf['COLLECTION_DATE']\n",
    "    thisDf['Hours_Collection_to_Accession'] = thisDf['Hours_Collection_to_Accession'].dt.total_seconds()/60/60\n",
    "    \n",
    "    thisDf['Bad_Accession_Date'] = thisDf['COLLECTION_DATE'] > thisDf['ACCESSION_DATE'] \n",
    "    thisDf['Bad_Accession_Date'] = thisDf['Bad_Accession_Date'].apply(lambda x: 1 if x is True else 0)\n",
    "\n",
    "    \n",
    "    return thisDf\n",
    "\n",
    "def second_drop_columns(thisDf):\n",
    "    return thisDf.drop(\n",
    "        [\n",
    "            'PERFORMING_LAB_LATITUDE',\n",
    "            'PERFORMING_LAB_LONGITUDE',\n",
    "            'ORDERING_LAB_LATITUDE',\n",
    "            'ORDERING_LAB_LONGITUDE',\n",
    "            'COLLECTION_DATE',\n",
    "            'ACCESSION_DATE',\n",
    "            'ADD_ON_ORDER_DATE',\n",
    "            'WORKLIST_CODE',\n",
    "            'PERFORMING_LAB_SITE_TYPE',\n",
    "            'ORDER_CODE', #new column used\n",
    "            #'PERFORMING_LAB_CODE',\n",
    "        ], axis=1)\n",
    "\n",
    "def get_dummy_vars(thisDf):\n",
    "    return pd.get_dummies(thisDf, drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "s3_bucket = 'dgx-ds-use1-dev-landing-s3/kamal/input'\n",
    "filename = 'orderdata_split.csv'\n",
    "data_location = 's3://{}/{}'.format(s3_bucket, filename)\n",
    "\n",
    "try:\n",
    "    #df = pd.read_csv('datathon_full.tab.zip', sep='\\t')\n",
    "    df = pd.read_csv(data_location)\n",
    "    print(df.shape)\n",
    "except Exception as inst:\n",
    "    print(inst)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first column cleanup\n",
    "df.drop(df.columns[[0,1]], axis=1, inplace=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples that will be used for development\n",
    "#df_t = df.sample(n=2000000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# initial data column drops\n",
    "df_t = initial_drop_columns(df)\n",
    "df_t = clean_data(df_t)\n",
    "\n",
    "#df.sample(100).to_csv('Sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_lat_lon = df_t[['PERFORMING_LAB_LATITUDE', 'PERFORMING_LAB_LONGITUDE', 'ORDERING_LAB_LATITUDE', 'ORDERING_LAB_LONGITUDE']]\n",
    "df_lat_lon.drop_duplicates(inplace=True)\n",
    "df_lat_lon = get_distances(df_lat_lon)\n",
    "\n",
    "df_t = pd.merge(\n",
    "    df_t,\n",
    "    df_lat_lon,\n",
    "    on=['PERFORMING_LAB_LATITUDE', 'PERFORMING_LAB_LONGITUDE', 'ORDERING_LAB_LATITUDE', 'ORDERING_LAB_LONGITUDE'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_t = clean_ordercode(df_t)\n",
    "#df.sample(100).to_csv('Sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance zscore\n",
    "#from scipy.stats import zscore\n",
    "#df['Distancez']=zscore(df['Distance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_t = do_date_stuff(df_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = update_add_on_exists(df_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = second_drop_columns(df_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an idea of how many new features we'll get from dummy explosion\n",
    "for col in list(df_t):\n",
    "    if (df_t[col].dtype =='object'):\n",
    "        print('col:', col, 'unique vals: ', df_t[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for nulls\n",
    "df_t.isna().any()\n",
    "#df['ACCNTYPE'].fillna('OTHER', inplace=True)\n",
    "#df_t.columns[df.isna().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df_t[df_t['TAT_HOUR'].isna()]\n",
    "dfx.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_t['TAT_HOUR'].fillna(round(df_t['TAT_HOUR'].mean()), inplace=True)\n",
    "df_t['ACCN_PROCESS_TYPE_CODE'].fillna('OTHER', inplace=True)\n",
    "df_t['MARKET_SEGMENT_DESC'].fillna('OTHER', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.head()\n",
    "#df_t.drop(df_t[[0]], inplace=True)\n",
    "#df_t.sample(1000).to_csv('Sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'dgx-ds-use1-dev-landing-s3' \n",
    "data_key = 'kamal/input/order_data_prep_job1.csv' \n",
    "comm_data_location = 's3://{}/{}'.format(bucket, data_key) \n",
    "df_t.to_csv(comm_data_location, index=False)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
